{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "853vgN24wZ6H"
      },
      "outputs": [],
      "source": [
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.randn(3, requires_grad=True) # use 'requires_grad' parameter to tell PyTorch to track the gradients\n",
        "print(x)\n",
        "\n",
        "y = x + 2 # this creates the computational graph (see below)\n",
        "print(y)\n",
        "# having printed 'y', we see that we get a grad_fn attribute that is 'AddBackward'\n",
        "# because our operation was a '+' and then backpropogation is done"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D6TcmQOWwp1J",
        "outputId": "0fb8828b-0fce-46a8-8c21-3ba527e97101"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.3790,  0.5939, -0.3639], requires_grad=True)\n",
            "tensor([2.3790, 2.5939, 1.6361], grad_fn=<AddBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- See image for **computational graph**:\n",
        "https://drive.google.com/file/d/\n1NemQC1_4kMYeVZBf-b9tEOiX84lW5Zfq/view?usp=share_link\n"
        "- each operation is a node with inputs and outputs\n",
        "- first we apply a forward pass in which we calculate the output y\n",
        "- due to the requires_grad parameter specified, PyTorch now goes ahead and automatically creates and stores a function for us that is used for back-propagation and to store the gradients"
      ],
      "metadata": {
        "id": "Ffj2rWHDyGpa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = y*y*2 # 'MulBackward' attribute\n",
        "print(z)\n",
        "\n",
        "z = z.mean() # applying a mean operation - 'MeanBackwards' attribute\n",
        "print(z)\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "z.backward() #this now calculates the gradients => it calculates dz/dx\n",
        "print(x.grad) # so now 'x' has a grad attribute, which is where the gradients are stored (returns a tensor of the gradients)\n",
        "\n",
        "# If 'requires_grad' was set to False (or left as the default False), and then we still had the 'z.backward()' line,\n",
        "# we would get an error saying \"element 0 of tensors does not require grad and does not have a grad_fn\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwKz5z_Mzhe8",
        "outputId": "6d4923be-5674-4cf1-f023-fc4c0b39edab"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([11.3190, 13.4565,  5.3538], grad_fn=<MulBackward0>)\n",
            "tensor(10.0431, grad_fn=<MeanBackward0>)\n",
            "   \n",
            "tensor([3.1720, 3.4585, 2.1815])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- In the background, this creates a vector Jacobian product to get the gradients.\n",
        "- We have the Jacobian matrix with partial derivatives and then we multiply this with a gradient vector and then we will get the final gradients that we are interested in **(chain rule)**:\n",
        "https://drive.google.com/file/d/1inA-FgcxuJfua9-T90OoFiAWU6mQ5z8s/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "8ntBraP_1vOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# in the above, our 'z' was a scalar since we applied the mean() operation\n",
        "# lets say we didn't apply the mean operation, now 'z' will have more than one value in it - of size (1,3)\n",
        "\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x + 2\n",
        "print(y)\n",
        "\n",
        "z = y*y*2 \n",
        "print(z)\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "# this next line produces an error: \"grad can be implicitly created only for scalar outputs\"\n",
        "#z.backward() \n",
        "\n",
        "# instead let's do this:\n",
        "v = torch.tensor([0.1, 1.0, 0.001], dtype=torch.float32)\n",
        "z.backward(v)\n",
        "print(x.grad)\n",
        "\n",
        "# the way this works is that in the background there is a vector Jacobian product between\n",
        "# the vector 'v' and 'z' - we needed to pass into the backward() function as 'z' wasnt a scalar\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_NrV2r-J3Bzu",
        "outputId": "6f5f8bff-cd57-4999-be4d-a83f185de813"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-1.5115, -0.7955,  0.0204], requires_grad=True)\n",
            "tensor([0.4885, 1.2045, 2.0204], grad_fn=<AddBackward0>)\n",
            "tensor([0.4773, 2.9018, 8.1637], grad_fn=<MulBackward0>)\n",
            "   \n",
            "tensor([0.1954, 4.8181, 0.0081])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To prevent PyTorch from tracking the gradients, we have three options:\n",
        "\n",
        "# 1)  call the requires_grad_() function and set it to False: x.requires_grad_(False)\n",
        "# 2)  call x.detach() - this creates a new tensor that doesnt require the gradient\n",
        "# 3)  write \"with torch.no_grad():\" and then do operations\n",
        "\n",
        "# Trying #1:\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "x.requires_grad_(False) # modified the 'requires_grad' variable in place\n",
        "print(x) # now we see that we have no grad attribute anymore\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "# Trying #2:\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "y = x.detach() # creates a new vector 'y' that has the same values but doesnt require a gradient\n",
        "print(y)\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "# Trying #3:\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "with torch.no_grad():\n",
        "  y = x + 2\n",
        "  print(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ILMycG4I5BV_",
        "outputId": "55b29f68-167c-41f2-978d-0a54e00460e4"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.4251, -2.2802, -0.3808], requires_grad=True)\n",
            "tensor([ 0.4251, -2.2802, -0.3808])\n",
            "   \n",
            "tensor([ 1.0346,  0.2069, -0.3713], requires_grad=True)\n",
            "tensor([ 1.0346,  0.2069, -0.3713])\n",
            "   \n",
            "tensor([-0.1199,  0.2346,  1.2646], requires_grad=True)\n",
            "tensor([1.8801, 2.2346, 3.2646])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Whenever we call the backward() function, the gradient for a tensor will be accumulated in the .grad attribute - so the values are being summed up."
      ],
      "metadata": {
        "id": "uY4ZwrJC7_Ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "for epoch in range(3):\n",
        "  model_output = (weights*3).sum()\n",
        "\n",
        "  model_output.backward()\n",
        "  print(weights.grad)\n",
        "\n",
        "  weights.grad.zero_() # with this we no longer have incorrect accumulating gradients\n",
        "\n",
        "  # with just one iteration (when we have range(1)), we get \"tensor([3., 3., 3., 3.])\"\n",
        "  # with range(2) we additionally have tensor([6., 6., 6., 6.])\n",
        "  # with range(3) we would get the total output to be:\n",
        "  #   tensor([3., 3., 3., 3.])\n",
        "  #   tensor([6., 6., 6., 6.])\n",
        "  #   tensor([9., 9., 9., 9.])\n",
        "\n",
        "  # for this reason, we need .grad.zero_() to zero out our gradients (zero out the 'grad' attribute)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iAAz9Zzh8M60",
        "outputId": "959ed7bc-94b2-4411-9217-fb8a346a7f9f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n",
            "tensor([3., 3., 3., 3.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- .backward() function is used to perform the backward pass of the computation in the computational graph.\n",
        "- When you call the .backward() function on a tensor, it triggers a backward pass through the computational graph, starting from that tensor and working backwards to compute the gradients of all the tensors that have requires_grad=True. The gradients are accumulated into the .grad attribute of each tensor.\n",
        "-  If you want to clear the gradients before each backward pass, you can use model.zero_grad() function which will set the gradients of all model parameters to zero."
      ],
      "metadata": {
        "id": "cUG16ZUIAnyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# later we will see PyTorch optimizers - lets say we have the following:\n",
        "\n",
        "weights = torch.ones(4, requires_grad=True)\n",
        "\n",
        "optimizer = torch.optim.SGD([weights], lr=0.01)\n",
        "optimizer.step() # do a optimizer step\n",
        "optimizer.zero_grad() # before going onto the next iteration, we must the '.zero_grad()' function"
      ],
      "metadata": {
        "id": "tauWs8-CClX6"
      },
      "execution_count": 24,
      "outputs": []
    }
  ]
}
