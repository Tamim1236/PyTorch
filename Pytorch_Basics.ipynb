{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Tensor Basics\n"
      ],
      "metadata": {
        "id": "GX-_Bu9Q5oT2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Y567BikX3VPs"
      },
      "outputs": [],
      "source": [
        "# in Pytorch, everything is a tensor of varying dimensions (1D tensor, 2D tensor, etc.)\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# x = torch.empty(1) # just an empty tensor - like a scalar value\n",
        "# x = torch.empty(3) # like a 1D vector with 3 elements\n",
        "x = torch.empty(2,3) # 2x3 tensor (matrix)\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A8Ra03C430jU",
        "outputId": "948196f7-6989-4cd8-bea3-7215314c25a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[3.8453e-34, 0.0000e+00, 3.3631e-44],\n",
            "        [0.0000e+00,        nan, 6.4460e-44]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,2)\n",
        "print(x) # tensor with random values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRyIJyaH4Ml4",
        "outputId": "05a0be79-31b1-4b27-e368-2679e95f02a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0338, 0.8626],\n",
            "        [0.3150, 0.4333]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# can specify with ones or zeros\n",
        "x = torch.ones(2,2)\n",
        "print(x)\n",
        "y = torch.zeros(2,2)\n",
        "print(y)\n",
        "\n",
        "print(x.dtype) # by default our values have a torch.float32 datatype"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo4qV5Yz4Sf8",
        "outputId": "c6a66c99-ffb8-48a9-a69e-082d2ec92377"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]])\n",
            "tensor([[0., 0.],\n",
            "        [0., 0.]])\n",
            "torch.float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.ones(2,2, dtype= torch.int) # we can use the dtype parameter to specify the data type of our values\n",
        "y = torch.ones(2,2, dtype= torch.double)\n",
        "z = torch.ones(2,2, dtype= torch.float16)  \n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDUnmOMf4lDb",
        "outputId": "7b4fbe12-fa62-490d-ee0b-29795d809d34"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1, 1],\n",
            "        [1, 1]], dtype=torch.int32)\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float64)\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also construct a tensor from data, like a Python list\n",
        "x = torch.tensor([1,2,3,4,5])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Uz0V25b5UjD",
        "outputId": "77a56d0a-7305-4a43-95c7-566260700d39"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1, 2, 3, 4, 5])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Basic Operations"
      ],
      "metadata": {
        "id": "pOa_Xnit5hs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "\n",
        "print(x)\n",
        "print(y)\n",
        "\n",
        "z = x + y\n",
        "print(z) # element-wise addition is done here\n",
        "\n",
        "z_2 = torch.add(x,y) # torch.add() is another way to add two tensors\n",
        "print(z_2)\n",
        "\n",
        "\n",
        "# We can also do an in-place addition:\n",
        "y.add_(x) # This will modify our y by adding all elements of x to y\n",
        "print(y)\n",
        "\n",
        "# In Pytorch, every function with a trailing underscore (such as add_() above) will do an in place operation,\n",
        "# thus modifying the variable it is applied on\n",
        "\n",
        "# We can also do z = x - y   or z = torch.sub(x,y)\n",
        "# To multiply we can do z = x*y  or z = torch.mul(x,y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f5ao_YeL5jz5",
        "outputId": "82d2047b-ae23-4fc0-aa02-31c4a3480f14"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4841, 0.8316],\n",
            "        [0.3370, 0.1803]])\n",
            "tensor([[0.8445, 0.3616],\n",
            "        [0.6615, 0.4312]])\n",
            "tensor([[1.3286, 1.1932],\n",
            "        [0.9985, 0.6115]])\n",
            "tensor([[1.3286, 1.1932],\n",
            "        [0.9985, 0.6115]])\n",
            "tensor([[1.3286, 1.1932],\n",
            "        [0.9985, 0.6115]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We can also do slicing operations on tensors, like we do with numpy arrays\n",
        "\n",
        "x = torch.rand(5,3)\n",
        "print(x)\n",
        "\n",
        "\n",
        "print(x[:1])    # all rows but only col 0\n",
        "print(x[1:])    # all columns, just first row\n",
        "print(x[1,1])   # the one at index (1,1)\n",
        "\n",
        "# if we have a tensor with one element, we can call the .item() method to get the actual value\n",
        "print(x[1,1].item())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GX9G4cDI6wr0",
        "outputId": "d454581e-865d-4a17-a8e5-2730350b0a79"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.9627, 0.9769, 0.0632],\n",
            "        [0.8108, 0.0534, 0.6111],\n",
            "        [0.1041, 0.2638, 0.9335],\n",
            "        [0.0791, 0.2503, 0.2257],\n",
            "        [0.7498, 0.8760, 0.1399]])\n",
            "tensor([[0.9627, 0.9769, 0.0632]])\n",
            "tensor([[0.8108, 0.0534, 0.6111],\n",
            "        [0.1041, 0.2638, 0.9335],\n",
            "        [0.0791, 0.2503, 0.2257],\n",
            "        [0.7498, 0.8760, 0.1399]])\n",
            "tensor(0.0534)\n",
            "0.05343818664550781\n",
            "tensor([[0.4991, 0.3225, 0.5195, 0.5421],\n",
            "        [0.6133, 0.6155, 0.5270, 0.8387],\n",
            "        [0.2724, 0.4646, 0.5342, 0.0812],\n",
            "        [0.1582, 0.2616, 0.4066, 0.6171]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape a tensor:\n",
        "x = torch.rand(4,4)\n",
        "print(x)\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "y = x.view(16) # .view() is used to reshape the tensor - num of elements should still remain the same\n",
        "print(y) # so now we just have a 1D vector with 16 elements all in this one dimension\n",
        "\n",
        "print(\"   \")\n",
        "\n",
        "z = x.view(-1,8) # here we are only specifying the second dimension and with '-1', Pytorch will automatically determine the right size\n",
        "print(z)\n",
        "print(z.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2Y-lNnt8V0D",
        "outputId": "491fbcbd-53c4-4e95-ae99-818982ffe386"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0836, 0.2379, 0.3597, 0.5504],\n",
            "        [0.0985, 0.7130, 0.2364, 0.3403],\n",
            "        [0.3458, 0.5428, 0.5341, 0.2795],\n",
            "        [0.9963, 0.5492, 0.6815, 0.7845]])\n",
            "   \n",
            "tensor([0.0836, 0.2379, 0.3597, 0.5504, 0.0985, 0.7130, 0.2364, 0.3403, 0.3458,\n",
            "        0.5428, 0.5341, 0.2795, 0.9963, 0.5492, 0.6815, 0.7845])\n",
            "   \n",
            "tensor([[0.0836, 0.2379, 0.3597, 0.5504, 0.0985, 0.7130, 0.2364, 0.3403],\n",
            "        [0.3458, 0.5428, 0.5341, 0.2795, 0.9963, 0.5492, 0.6815, 0.7845]])\n",
            "torch.Size([2, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Converting between numpy arrays and tensors\n",
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5)\n",
        "print(a)\n",
        "print(a.type)\n",
        "\n",
        "print(\"       \")\n",
        "\n",
        "b = a.numpy()\n",
        "print(b)\n",
        "print(type(b))\n",
        "\n",
        "print(\"       \")\n",
        "\n",
        "# !! If the tensor is on the CPU and not the GPU, then both objects will share the same memory location, which means that\n",
        "# if we were to change one, we would also change the other\n",
        "\n",
        "# let's show the point above:\n",
        "a.add_(1) # in place addition to add a 1 to each element\n",
        "print(a)\n",
        "print(b)  # we can see that even b (the numpy array version of 'a') was modified from the modification of 'a' (our tensor) \n",
        "          # since they both point to the same memory location!\n",
        "\n",
        "print(\"       \")\n",
        "\n",
        "#  lets do it the other way: converting from numpy array to torch tensor\n",
        "\n",
        "a = np.ones(5)\n",
        "print(a)\n",
        "b = torch.from_numpy(a)\n",
        "print(b)\n",
        "\n",
        "print(\"       \")\n",
        "\n",
        "# We show the point above once again - since memory is shared, modification in one also modifies the other\n",
        "a += 1\n",
        "print(a)\n",
        "print(b)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT7h_hqe86ej",
        "outputId": "a94975d1-79cc-42c8-e9e3-2fc69ba53e2d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "<built-in method type of Tensor object at 0x7fc67b172a40>\n",
            "       \n",
            "[1. 1. 1. 1. 1.]\n",
            "<class 'numpy.ndarray'>\n",
            "       \n",
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n",
            "       \n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([1., 1., 1., 1., 1.], dtype=torch.float64)\n",
            "       \n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# need cuda toolkit to use GPU\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  x = torch.ones(5, device = device) # to create a tensor and put it on the GPU\n",
        "\n",
        "  y = torch.ones(5)\n",
        "  y = y.to(device) # to move it to our GPU device\n",
        "\n",
        "  z = x + y # if we now do this operation it will be performed on the GPU and will probably be much faster as a result\n",
        "  \n",
        "  z.numpy() # but this will return an error because numpy can only handle CPU tensors (cannot convert a GPU tensor back to numpy)\n",
        "  z = z.to(\"cpu\") # so we should move it back to the CPU"
      ],
      "metadata": {
        "id": "c0dTljGj-rbm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We often see the following:\n",
        "x = torch.ones(5, requires_grad=True) # by default 'requires_grad=False'\n",
        "print(x)\n",
        "\n",
        "# this tells Pytorch that we will need to calculate the gradients for this tensor later in our optimization steps\n",
        "# so whenever we have a variable in our model that we need to optimize, we need the gradients, so we should specify\n",
        "# 'requires_grad = True'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1Fi7KnT_c2d",
        "outputId": "4cd0481c-6353-4d7d-fee6-90f1d465617e"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.], requires_grad=True)\n"
          ]
        }
      ]
    }
  ]
}