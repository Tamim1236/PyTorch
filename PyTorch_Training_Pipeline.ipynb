{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "The training pipeline typically consists of three steps:\n",
        "- 1) Design the model with (input, output size, forward pass)\n",
        "- 2) Construct loss and optimizer\n",
        "- 3) Training loop (iterate through this a few times until we are done): \n",
        "    - forward pass: compute the prediction\n",
        "    - backward pass: compute gradients\n",
        "    - update weights\n",
        "  "
      ],
      "metadata": {
        "id": "r8teEFu_YbO9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vp8g9-KTYKEx",
        "outputId": "95fe0ef6-cb90-4b3f-92e9-bce1a2350c14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 1\n",
            "Prediction before training is f(5) = 0.188\n",
            "epoch 1: w = 0.193, loss = 26.51708603\n",
            "epoch 11: w = 1.379, loss = 0.90803540\n",
            "epoch 21: w = 1.579, loss = 0.23254812\n",
            "epoch 31: w = 1.621, loss = 0.20290354\n",
            "epoch 41: w = 1.637, loss = 0.19067675\n",
            "epoch 51: w = 1.648, loss = 0.17956752\n",
            "epoch 61: w = 1.659, loss = 0.16911556\n",
            "epoch 71: w = 1.669, loss = 0.15927213\n",
            "epoch 81: w = 1.679, loss = 0.15000163\n",
            "epoch 91: w = 1.688, loss = 0.14127083\n",
            "Prediction after training is f(5) = 9.375\n"
          ]
        }
      ],
      "source": [
        "# continuing the code from the \"PyTorch_Gradient_Descent\" file\n",
        "import torch\n",
        "import torch.nn as nn # importing the neural network module\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "print(n_samples, n_features)\n",
        "\n",
        "\n",
        "# model needs input and output sizes\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "model = nn.Linear(input_size, output_size) # but now we need X and Y to have a 2x2 shape\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "print(f'Prediction before training is f(5) = {model(X_test).item():.3f}') # passing in the test sample into the model\n",
        "# .item() used to get the actual value of the one item that is there\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate) # instead of weights we just need model.parameters()\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # first we need the prediction (through the forward pass)\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # now we need the loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # now we need to do the backward pass\n",
        "  l.backward() # this will compute the gradient of our loss w.r.t 'w'\n",
        "\n",
        "  optimizer.step() #does an optimization step instead of the manual weight updating we did previously\n",
        "\n",
        "  # zero the gradients\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0: # printing a message at every other epoch\n",
        "    [w,b] = model.parameters() # the model weights along with the bias\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}') # get the first weight with w[0][0]\n",
        "\n",
        "print(f'Prediction after training is f(5) = {model(X_test).item():.3f}') # passing in the test sample into the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In our model above, we just had one layer (the 'nn.Linear()'), but for more complex models consisting of multiple\n",
        "# layers, we can do the following:\n",
        "\n",
        "# create a custom class for our model representation:\n",
        "\n",
        "class LinearRegression(nn.Module): # derived from nn.Module\n",
        "  def __init__(self, input_dim, output_dim):\n",
        "    super(LinearRegression, self).__init__() # calling the super-constructor\n",
        "    # now define layers:\n",
        "    self.lin = nn.Linear(input_dim, output_dim)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "  \n",
        "\n",
        "model = LinearRegression(input_size, output_size)\n",
        "\n",
        "# Now let's test it out (using the code we wrote for the last model above)\n",
        "\n",
        "X = torch.tensor([[1],[2],[3],[4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2],[4],[6],[8]], dtype=torch.float32)\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# model needs input and output sizes\n",
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "X_test = torch.tensor([5], dtype=torch.float32)\n",
        "print(f'Prediction before training is f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "n_iters = 100\n",
        "\n",
        "loss = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  y_pred = model(X)\n",
        "\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  l.backward() \n",
        "  optimizer.step() \n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 10 == 0: \n",
        "    [w,b] = model.parameters() \n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:.8f}')\n",
        "\n",
        "print(f'Prediction after training is f(5) = {model(X_test).item():.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtI1cszS9kUS",
        "outputId": "20bde15a-d48c-4429-cd56-48ea2a3c5938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training is f(5) = -3.312\n",
            "epoch 1: w = -0.310, loss = 52.03084564\n",
            "epoch 11: w = 1.349, loss = 1.50452352\n",
            "epoch 21: w = 1.624, loss = 0.18806896\n",
            "epoch 31: w = 1.676, loss = 0.14532833\n",
            "epoch 41: w = 1.693, loss = 0.13604680\n",
            "epoch 51: w = 1.703, loss = 0.12810701\n",
            "epoch 61: w = 1.712, loss = 0.12064995\n",
            "epoch 71: w = 1.720, loss = 0.11362746\n",
            "epoch 81: w = 1.729, loss = 0.10701378\n",
            "epoch 91: w = 1.737, loss = 0.10078500\n",
            "Prediction after training is f(5) = 9.472\n"
          ]
        }
      ]
    }
  ]
}